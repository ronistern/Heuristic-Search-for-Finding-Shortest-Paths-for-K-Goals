For the rest of this paper, we assume that the heuristics $h_1,\ldots, h_k$ are consistent.
[[AF: then why above did you also assume inconsistent. Maybe assume this for the entire paper and delete everything that has to do with inconsistent heuristics. The paper is too long with details anyway.]]
Thus, according to Theorems~\ref{thm:consistent} and~\ref{thm:admissible}, when a goal $t_i$ is expanded by either \kastarmax or \kastarmin we are guaranteed that the optimal path to it has been found.
[[AF: both only mentioned above but not yet defined and the terms were not yet defined and used. Theorems 16 and 18 were not yet given. Try to re-word or to bring the necessary definitions before this claim (or move this to afterwards]]
This allows us to safely remove $t_i$ from the set of active goals (see line~\ref{line:removeGoal} in Algorithm~\ref{alg:kastar}).
Removing a goal from the set of active goals affects how the $F$ values is computed for states generated in subsequent \kastar iterations, as only heuristics for the remaining active goals will be considered.
Thus, a state may have two different $F$ values associated with it: the $F$ according to which it is ordered in \open and the $F$ value computed according to the current set of active goals.
When these $F$ values are different we refer to the former $F$ value as \emph{stale} and refer to the latter $F$ value as the \emph{up-to-date} $F$ value. [[AF: you must reorder OPEN now]]
% However, the states in \open now have \emph{stale} $F$ values, in the sense that the $F$ value according to which they are stored in \open  are different from their current $F$ values, if these were computed now (i.e., w.r.t to current set of active goals).
%if we would compute their $F$ values now it will be different from  the $F$ value stored for it in \open
%their $F$ values were computed when the set of active goals was different.
%\begin{definition}[Stale]
%    A state $n$ has a stale $F$ value if the set of active goals considered when that $F$ value was computed
%    is different from the $F$ value of that state when considering the current set of active states.
%\end{definition}

%is beneficial because state generation from here on will be cheaper, as there is no point in computing the heuristic function for goals that are no longer active.

%Removing a goal from the set of active goals has several potential effects. First, computing the state evaluation function for any state generated from here on will be cheaper, as there is no point in computing the heuristic function for goals that are no longer active.

Considering stale $F$ values provides poor guidance to the search, directing it towards goals that are no longer active.
For example, consider the \kgs depicted in Figure~\ref{fig:lazy}.
The edge weights are depicted next to the edges.
The shortest path to $t_1$ and $t_2$ do not go through any other intermediate node and cost 3 and 1 respectively.
After $s$ is expanded the nodes in \open are $t_1$, $t_2$, $A$, and $C$.
Assume that the heuristics $h_1$ and $h_2$ are perfect and that we use $\min$ as the heuristic aggregation function. [[AF: just fully define Fmin and Fmax above.]]
Therefore the $F$ values are as given in the Table.
Thus, $t_2$ is expanded next.
At this stage \open contains the states $t_1$ with $F(t_1) = 3$, $A$ with $F(A) = 2$, and $C$ with $F(C) = 9$.
So $A$ would be expanded, followed by expanding $t_1$ and halting.
Note that after $t_2$ was expanded, the $F$ values of $t_1$, $A$, and $C$ were stale.
They were computed when $t_2$ was active, and had we re-computed the $F$ values of $t_1$, $A$, and $C$ after $t_2$ was expanded, their $F$ values would change to $F(t_1) = 3$, $F(A) = 4$, and $F(C) = 9$.
Using these up-to-date $F$ values, $t_1$ would have been expanded after $t_2$ and before $A$.
At this stage the search would halt, returning the optimal paths to $t_1$ and $t_2$ without ever expanding $A$ nor $C$.
Indeed, $A$ and $C$ are surplus states (Definition~\ref{def:surplus}) with respect to both $t_1$ and $t_2$.
Thus, considering stale $F$ values can introduce inefficiency to the search.

[[AF: nice example. But, the text has too much past tense "would have been etc". Try to simplify the text]]


\begin{figure}
  \centering
  \subfloat{
    \begin{tikzpicture}
    \node[source]  (s)  at (2, 2) {$S$};
    \node[other]   (a)  at (4, 0) {$A$};
    \node[target2] (t2) at (4, 2) {$t_2$};
    \node[other]   (c)  at (0, 0) {$C$};
    \node[target1] (t1) at (2, 0) {$t_1$};

    \draw[->] (s) -- (a)  node[midway] (sa) {\phantom{0}};
    \draw[->] (s) -- (t2)  node[midway] (st2) {\phantom{0}};
    \draw[->] (s) -- (c)  node[midway] (sc) {\phantom{0}};
    \draw[->] (s) -- (t1) node[midway] (st1) {\phantom{0}};
    \draw[->] (a) -- (t1) node[midway] (at1) {\phantom{0}};
    \draw[->] (a) -- (t2) node[midway] (at2) {\phantom{0}};

    \node[above right = -5mm of sa,edgeweight] {$1$};
    \node[above left = -5mm of sc,edgeweight] {$3$};
    \node[left  = -2mm of st1,edgeweight] {$3$};
    \node[below = -2mm of at1,edgeweight] {$3$};
    \node[above = -2mm of st2,edgeweight] {$1$};
    \node[right = -2mm of at2,edgeweight] {$1$};

    \node[left  = 2mm of  s,infobox] {$\vect{h}=\vect{0}$};
    \node[below = 0mm of  a,infobox] {$\vect{h}=\tuple{3, 1}$};
    \node[below = 0mm of  c,infobox] {$\vect{h}=\tuple{9, 6}$};
    \node[below = 1mm of  t1,infobox] {$\vect{h}=\tuple{0, 9}$};
    \node[right = 2mm of  t2,infobox] {$\vect{h}=\tuple{9, 0}$};
  \end{tikzpicture}
  } \hfill
  \subfloat{
    \begin{tabular}[b]{lccccc}
      \toprule
      Path & $g$ & $\Phi_{t_1, t_2}(\vect{h})$ & $F$ & $\Phi_{t_1}(\vect{h})$ & $F$\\
      \midrule
      $St_2$  & $1$ & $0$ & $1$ & & \\
      $SA$    & $1$ & $1$ & $2$ & $3$ & $4$\\
      $St_1$  & $3$ & $0$ & $3$ & $0$ & $3$\\
      $SAt_1$ & $4$ & $0$ & $4$ & $0$ & $4$\\
      $SAt_2$ & $2$ & $0$ & $2$ & & \\
      $SC$    & $3$ & $6$ & $9$ & $9$ & $12$\\
      \bottomrule
    \end{tabular}
  }
  \caption[Illustrating the pitfalls of not reordering \open.]{Example illustrating the pitfalls of not reordering \open, using $\Phi = \min$.
  An example where running \kastarmin without reordering \open after a goal is expanded results in expanding a surplus state ($A$).

  Running Lazy \kastarmin recomputes the value of $A$ after expanding $t_2$, but avoids expanding $A$ ; \kastarmin does not recompute the value of $C$.}
  \label{fig:lazy}
\end{figure}

%sec:smart-eager

\subsection{Reordering \open eagerly}
\label{sec:eager}
One option to avoid using stale $F$ values is to completely reorder \open after removing a goal from the set of active goals (line~\ref{line:removeGoal}).
This is done by re-computing the state evaluation function (\texttt{ComputeF}) for all states in \open to get their up-to-date $F$ value, and then reordering \open accordingly.
We refer to this option as \emph{Eager \kastar}. [[AF: I of course like the name. Again, use other names for the other variants above.]]
Using Eager \kastar can be costly, as it requires reordering \open $k-1$ times. 
We analyze this runtime overhead incurred due to these reorderings in Section~\ref{sec:resource-analysis}.

Observe that the $F$ value of some states does not become stale when a goal is removed from the set of active goals.
For example, consider a \kgs instance with 3 goals and a state $n$ with $f$ values $\vect{f}(n)= \tuple{10, 5, 3}$.
When using \kastarmin, $F(n)=f_3(n)=3$.
Therefore, the $F$ value of $n$ will only change if the goal $t_3$ is removed from the set of active goals.
In particular, the $F$ value of $n$ does not become stale when either $t_1$ or $t_2$ are removed from the set of active goals, as this does not change the (up-to-date) $F$ value of $n$. [[AF: You might want to say that if F=3 then the paths to the other nodes was found with $f \leq 3$ and this is why this node is still in OPEN]]
Consequently, there is no point in to re-computing $F(n)$ when goal $t_1$ or $t_2$ are expanded. More generally, after a goal $t_i$ is expanded there is no need to re-compute the $F$ value of a state if it is not stale  [[AF: there is a need to recompute only for stale nodes]]. %that iof any state for which $F(n)\neq f_i(n)$.
%expanded there is no need to re-compute the $F$ value of any state for which $F(n)\neq f_i(n)$.

%[[AF: how about principal goal? or key goal?]]
We implemented this optimization by storing additional information for each node $n$, keeping track of the goal $t_i$ for which $F(n)=f_i(n)$.
We refer to this goal as the \emph{responsible goal} of $n$.
If there are multiple responsible goals, we choose one arbitrarily.
%[[AF: Why? choose them all. Only when it is empty you should re-order open]]
Then, we re-compute the $F$ value of a node and re-insert [[change its priority] it into \open only if its responsible goal is removed now from the set of active goals. [[AF: if you have two responsible goals and one is removed there is no need to recalculate its value. Of course, you will need to store a list of responsible goals. In fact, you can store a vector of k bits and set the ones that are responsible. Only when this vector is all 0 you need to recalculate the F-value. Outch, this is the version below]][[Roni and Meir: we only keep one responsible node. One could keep a list of responsible, but that seems overly complex. Meir might try anyhow to implement this]]
This version of Eager \kastar is referred to as Eager$^+$.
%[[AF: I would not call it eager+. This is an implementation trick for eager and should not have its own sign.]] [[AF: you do not only recompute. You also change its place in \open. If there are too many of these, why not sort \open from scratch?]]

A more involved implementation would keep track of the set of responsible goals for every state.
I.e., storing for every state $n$ the set of goals for which $f(n)=F(n)$.
Then, when a goal $g_i$ is expanded, we iterate over every state that had $g_i$ in its set of responsible goals and remove $g_i$ from that set.
Then, a state will only re-compute its $F$ value (and re-position it in \open accordingly) if its set of responsible goals is empty.
This approach, however, requires storing more information per state and is more complex, and so we implemented only Eager$^+$ in our experimental evaluation (Section~\ref{sec:experimentalResults}). [[AF: call the new one Eager++?]] %[[AF: OK, you answered me. But, I think this should be the correct way to do eager. I do not like eager+ not in its name and not in its content. By the end of the day, the responsible goal is out of the computation so it does not really matter.]]
%former approach, which keeps track of a single responsible goal.

%[[AF: the last sentence is not understandable. you keep the set for who? Which nodes? For everybody? please better explain what you mean. I did not understand. Also, consider to delete this all together. You did not implement it so why bother? Maybe in a footnote? But, please better explain for sure.]]

%[[AF: I would delete all the non eager+ from the experiments]]

% Lazy!

[[ARIEL GOT HERE. 24/06/18 at 17:05]]

\subsection{Lazy \kastar}
% We outlined above an example where the cost of reordering \open after a goal is expanded does not [[AF: I think you want to delete the "not"]]incur significant overhead [[AF: where did you outline this? You just showed that you must reorder open, otherwise, you have more node expansions]]. However, this is not always the case. For example, in domains where the search frontier does not grow exponentially, then states generated after the $k-1$ goal has been expanded are not larger than the states generated up to that point. Indeed, we show in our experimental results that in some domains reordering does, in fact, incur a significant amount of runtime.
%[[AF: you just said that it is negligible. Please explain why this is not the case in these domains. Are they polynomial?]]

%[[AF: Roni will reword and say that eager can be costly.]]

Both Eager \kastar [[AF: you mean without the responsible goal??, define this]]and Eager$^+$ \kastar will re-compute the $F$ value and reorder [[AF: reorder is used when the entire list is reoredered. You mean that we will have to apply some kind of change-priority for these nodes. Minor issue.]] all the states in \open that have stale $F$ values.
This incurs a cost that can be significant, as we observed in our experimental evaluation.
To this end, we propose an alternative approach in which states are re-evaluated and repositioned in \open in a lazy manner.
This means we re-compute a state's $F$ value only if its responsible goal is not active \emph{and} it is selected for expansion (line~\ref{line:open:chooseNode} in Algorithm~\ref{alg:kastar}).
Then, we re-insert it into \open only if its $F$ value has increased and it no longer has the minimal $F$ value in \open.


We call this algorithm Lazy \kastar, since it is directly inspired by the Lazy \astar algorithm~\cite{betzalel2015typeSystem,tolpin2013toward}, where multiple heuristics are used towards the same goal, and are evaluated lazily in a similar manner.
Lazy \kastar behaves differently when using \maxf than when using \minf. [[AF: never defined]]
Thus, we analyze them separately.
%[[AF: I already said that I think these terms are redundant. Your call]]\roni{I don't see a better way to discuss the different F functions then to name them}

%n particular, the lazy approach only brings benefit to \kastarmin

\subsubsection{Lazy \kastarmin}

Consider running Lazy \kastarmin [[AF: also never defined]] on the graph depicted in Figure~\ref{fig:lazy}.
Lazy \kastarmin behaves exactly as Eager \kastarmin until the first goal is expanded, which in this case is $t_2$.
At this stage, \open contains $t_1$, $A$, and $C$ with $F$ values 3, 2, and 9, respectively.
The $F$ values of $A$ and $C$ are stale, and therefore Eager$^+$ \kastarmin (and also Eager \kastarmin) will re-compute their $F$ values and re-insert them into \open.
Lazy \kastarmin does not do so.
Instead, it continues with these stale $F$ values, selecting $A$ from \open.
Lazy \kastarmin then observes that $F(A)$ is stale and re-computes it.
The updated $F$ value of $A$ is 4, which is greater than the $F$ value of $t_1$, 3.
So, $A$ is re-inserted into \open and the next state selected from \open is $t_1$.
After expanding $t_1$ the search halts, having an optimal path to both goals.
As we can see, Lazy \kastarmin did not re-compute the $F$ value of $C$ although it was stale.
Nonetheless, Lazy \kastarmin did not expand more states than Eager \kastarmin.

More generally, the following Lemma shows that although Lazy \kastarmin does not reorder all the states in \open when a goal is expanded and thus its \open may contain states with stale $F$ values,
it still manages to expand states in order of their up-to-date $F$ values. % (i.e., according to the $F^u$ value).

%This occurs not just in this example, but in generalWe now prove this observation to the  that this is true not just in this example but in general.

% As a result, $n_2$  and re-position it in \open so that the  Note that the responsible goal for $n_3$ is $g_2$, so when $g_2$ is expanded both Eager \kastarmin and Eager$^+$ \kastarmin will re-compute the $F$ value of $n_3$ (updating it to 16) and re-position it in \open accordingly. Running Lazy \kastarmin will not do so, since  i \kgs our running example from Figure~\ref{fig:need-resort}.  Lazy \kastarmin behaves exactly as Eager \kastarmin until the first goal is expanded, which in this case is $g_2$. Recall that at this stage \open consists $g_1$ and $n_2$, having $\textbf{f}(g_1)=(6,15)$ and  $\textbf{f}(n_2)=(7,5)$. Since $g_1$ and $n_2$ were inserted into \open when both $g_1$ and $g_2$ were active, their $F$ values are $F(g_1)=6$ and $F(n_2)=5$. Both $F$ values are stale, as  if we computed them now (after $g_2$ was removed from the set of active goals), we will have $F(g_1)=6$ and $F(n_2)=7$. Using Eager (or Eager), and are thus stale,  since $g_2$ is now removed from the set of active goals and thus  $F_min().
%We now prove that although Lazy \kastarmin does not reorder all the states in \open when a goal is expanded, it still expands states in order of their up-to-date $F$ values. % (i.e., according to the $F^u$ value).
% Lazy with minf is good
\begin{lemma}[Lazy \kastarmin is a best-first search]
Lazy \kastarmin always expands the state in \open with the smallest up-to-date $F$ value. % $F^u$
%Every path $\pi_i$ returned by Lazy \kastarmin is guaranteed to be the lowest-cost path to its corresponding goal ($g_i$).
\label{the:lazy-minf-correct}
\end{lemma}
\begin{proof}

%[[AF: the same sentence is repeated three times.]]
At a given point in time, a state $n$ may have a stale $F$ value (according to which it is positioned in \open), and an up-to-date $F$ value.
To differentiate between them, we denote the former by $F(n)$ and the latter by $F^u(n)$.
Lemma~\ref{the:lazy-minf-correct} thus states that if $n$ is expanded by Lazy \kastarmin then $F(n)=\min_{n'\in \open}F^u(n')$. [[Define this once and present the lemma only once; no need for repetition]]
We prove this by induction on the iterations of the main loop of Lazy \kastarmin.
%, showing that Lazy \kastarmin and Eager \kastarmin expand exactly the same set of states.
The base of the induction is trivial: in the first step $F(s)=F^u(s)$ and \open contains only $s$.
Now assume that in the first $m-1$ iterations, Lazy \kastarmin expanded the state with the smallest $F^u$ value, and consider the $m^{th}$ iteration of Lazy \kastarmin.

First, observe that $F(n)\leq F^u(n)$ [[AF: I would write: $F^u(n)\geq F(n)$ ]for every state $n$.
This is because the $F$ value in \kastarmin is the minimum over the $f$ values for the active goals and the set of active goals when $F$ was computed is either the same or a superset of the current set of active goals (according to which $F^u(n)$ is computed).

Now, let $n$ be the state selected for expansion from \open in the $m^{th}$ iteration of Lazy \kastarmin. Hence, it had the smallest $F$-value in \open.
In addition, $F(n)=F^u(n)$, as otherwise it would have been re-inserted into \open.
Therefore
%\[ \forall n'\in\open ~~ F^u_{min}(n)=F_{min}(n)\leq F_{min}(n') \leq F^u_{min}(n') \]
\[ \forall n'\in\open ~~ F^u(n)=F(n)\leq F(n') \leq F^u(n') \]
  So, $n$ has the smallest $F^u$ value in \open, as required.
\end{proof}

[[AF: proof is trivial. You might want to omit it to gain space]

The implication of Lemma~\ref{the:lazy-minf-correct} is that both Lazy \kastarmin and Eager \kastarmin expand states according to their $F^u$ values, except that Lazy \kastarmin does so more efficiently, without the need to reorder all the stale states in \open.
% [[AF: So you do reorder the entire open list. with Eager. Or do you only insert these nodes that their F values was changed?]]

However, the exact sets of states they expand may be different due to tie breaking.
For example, consider a tie-breaking rule that chooses the state that has the smallest sum of $f$ values (among the states with the same $F$ value).
Now, assume that we have two states in \open, $n_1$ and $n_2$, with $\mathbf{f}$ values $\tuple{5, 6, 10 }$ and $\tuple{5, 7, 7}$, respectively.
They have the same $F$ value (5), but according to this tie-breaking rule \kastarmin will choose $n_2$ before $n_1$.
Now, assume that $t_3$ was expanded.
Using Eager \kastarmin would result in $n_1$ now being expanded before $n_2$, while Lazy \kastarmin would not do so.
Since tie-breaking rules can make a significant difference in performance~\cite{asai2017tieBreaking}, this means that Eager \kastarmin may still outperform Lazy \kastarmin.
In our experimental results, however, this did not occur and Lazy \kastarmin was in general better. %[[AF: Tie breaking can go either way. You might want to delete all this altogether]]\roni{I don't follow}

[[AF: Tie breaking can go either way. That is, tie breaking can favor each of them and probably on average does not favor anyone more than the other. Thus, you might want to delete this discussion all this altogether, as tie breaking did not provide any surprising experimental results.]]
% Lazy does not work for Max-f, but  does work on Min-f
\subsubsection{Lazy \kastarmax}

%Using Lazy \kastarmin has significant impact [[AF: what do you mean, runs faster? explain why?]] in practice. Unfortunately,
Lemma~\ref{the:lazy-minf-correct} does not transfer to Lazy \kastarmax.
%Using the Lazy approach for \kastarmax does not have the same effect.
In fact, we show next that Lazy \kastarmax is equivalent to running \kastarmax that does not reorder \open when a goal is expanded.
This results in Lazy \kastarmax expanding states with stale $F$ value, even if there are other states in \open whose up-to-date $F$ values are smaller.
%In fact, Lazy \kastarmax is equivalent to running \kastarmax without reordering \open after a goal is expanded.
%,and thus it may expand states that do not have the lowest updated $F$ value in \open.
%[[AF: who is not doing the reordering. Ambiguous. Please precisely say what you mean]].
%[[AF: recall? I do not remember anything??]]
As discussed above (and shown in the example in Figure~\ref{fig:lazy}), this is a negative result since it means that Lazy \kastarmax will be guided towards goals even after the optimal path to them has been found.
[[AF: so this algorithm is no-good and should not be used. Do you want to prove theorems on that?]]
% we cannot apply the lazy approach to \kastarmax and preserve admissibility.
\begin{theorem}%[Lazy \kastarmax is not a best-first search]
  Lazy \kastarmax expands exactly the same set of states as \kastarmax, without reordering \open.
  \label{the:lazy-maxf-bad}
\end{theorem}

[[Delete the comma. Also, I think that you can call it by name KA*MAX-Basic or something similar. It is very confusing to jump between variants the way you do it]]

\begin{proof}
  To prove Theorem~\ref{the:lazy-maxf-bad}, we show that if Lazy \kastarmax selects a state from \open it will never decide to insert it back in \open.
  Let $n$ be a state with the minimal $F$ value in \open.
  Lazy \kastar decides to re-insert $n$ iff both conditions hold: (1) $F(n)$ is stale, and
  (2) there is a state $n'$ in \open for which $F(n')<F^u(n)$. %whose $F$ value is smaller than the up-to-date $F$ value of $n$.
  Assume, by negation, that such a state $n'$ exists.
  Since $n$ was selected for expansion before $n'$, we have that $F(n)\leq F(n')$.
  Now, observe that when using \maxf to compute $F$ values, then $F^u(m)\leq F(m)$ for every state $m$.
  Thus, $F^u(n)\leq F(n) \leq F(n')$, contradicting the assumption that $F(n')<F^u(n)$.
\end{proof}
%Consider Figure~\ref{fig:lazy}(a). First $s$ is expanded, generating $n_1$ and $n_2$ with ${\textbf{f}}(n_1)=(10,3)$  and ${\textbf{f}}(n_2)=(7,7)$. Thus, $n_2$ will be expanded, generating $g_1$ and $g_2$, each with a $g$ value of 7.  Assume that the heuristic of $g_1$ and $g_2$ are both zero (for both goals). Then \kastarmax will expand next $g_1$ and $g_2$, never expanding $n_1$ and returning a suboptimal path to $g_2$ of cost 7.

[[AF: ]The main point is that with MAX the f-value can only go down and this will not push it deeper into open.]]

% Using Lazy \kastarmin has significant impact in practice, as it decides in many cases to re-insert into \open the state with the minimal $F$ value. This is not the case in Lazy \kastarmax. In fact, Lazy \kastarmax will \emph{never} re-insert a state into \open after re-computing its $F$ value.
%To better understand why Lazy \kastarmax This is because re-computing an $F$ value for a state after the set of relevant goals has been reduced will only cause the $F$ value to decrease. Thus, is a state already has the smallest $F$ value in \open, it will still have the smallest $F$ value after we re-compute its $F$ value.
For this reason, we implemented only the two Eager approaches (Eager and Eager$^+$) for \kastarmax.

%[[AF: not clear. You never used lazy? What two approaches. This entire section needs some more rewording as I said. I think I do want to see this section 5 again.]]

%[[AF:  As I said above, I think we only need eager and the exact implementation is less important. That is, I do not like the term eager+. Your call]]


\subsubsection{Lazy for General Aggregation Functions}
[[TODO: New subsection to read]]
Next, we ask the question of which heuristic aggregation functions can use Lazy \kastar and obtain  an admissible results (i.e., optimal paths to all goals). [[AF: I would summarize that eager and eager+ are always good. Lazy is OK for min but not for max. And now, we want to know what further with other functions]]
To this end, we somewhat abuse [[AF: is this the correct word? You now use it on any size of vector up to k. Maybe you are widening it? Generalizing?]] the previous notation by defining $\Phi$ as an aggregation function that can accept vectors of different sizes,  specifically any size between 1 and $k$.
Common aggregation functions such as $\max$, $\min$, and average are all such functions, as we can apply them to vectors of different sizes. 

\begin{definition}
We say that a heuristic aggregation function $\Phi$ is \emph{isotone} if for any two sets $\activeg' \subseteq \activeg$ and for any vector $\vect{v}$, it holds that $\Phi_{\activeg'}(\vect{v})\leq \Phi_{\activeg}(\vect{v})$.
Conversely, $\Phi$ is called \emph{antitone} if $\Phi_{\activeg'}(\vect{v})\geq \Phi_{\activeg}(\vect{v})$ for any sets $\activeg'\subseteq \activeg$ and vector $\vect{v}$.
\end{definition}

%[[AF: you mean that with a subset it never gets larger. The term "monotonically" usually means that as time passes and something progresses. Here, you delete items. So, maybe use a different term. Something from set theory. I do not have anything in mind. Maybe reverse the term: if when adding more items you may get a smaller value you are monotonically non-increasing and vise versa]][[AS: How about now?]]

The $\min$ and $\max$ operators are trivial examples of, respectively, antitone and isotone aggregation functions.
%[[AF: add that Max and Min are trivial examples of non-increasing/]]
%We now provide sufficient conditions for identifying an aggregation function that can be used with Lazy \kastar, and when these conditions results in an effective use of Lazy \kastar.
\begin{theorem}
  If a heuristic aggregation function $\Phi$ is antitone then Lazy \kastarphi always expands the state with the smallest up-to-date $F$ value.
\end{theorem}
\begin{proof}
Let $\Phi$ be an antitone heuristic aggregation function, $n$ be a node chosen for expansion, and $n'$ be some other node in \open.
Now, assume that the $F$ value of $n'$ was computed w.r.t. a set of active goals $\activeg$ and that the current set of active goals is $\activeg'\subset \activeg$.

Node $n$ is chosen for expansion so its $F$ value is up-to-date, so $F(n)=g(n)+\Phi_{\activeg'}(\vect{h}(n))$, and also the smallest in \open, so $F(n)\leq F(n')=g(n')+\Phi_{\activeg}(\vect{h}(n'))$.
Since $\Phi$ is antitone, $F(n')\leq F^u(n')$, as required.  [[AF: intuitively, for a subset of goals the aggregated value can on only increase so choosing it now as the minimal node in open is safe.]]
\end{proof}

\begin{observation}
  \label{obs:genMaxNoLazy}
  If a heuristic aggregation function $\Phi$ is isotone then Lazy \kastarphi expands exactly the same set of nodes as \kastarphi without reordering \open.
\end{observation}
Observation~\ref{obs:genMaxNoLazy} is a straightforward generalization of Theorem~\ref{the:lazy-maxf-bad},
since for every $n$ having the minimal $F$ value and a set of active goals $\activeg$ and its subset $\activeg'$ it holds that $F(n)=g(n)+\Phi_{\activeg}(\vect{h}(n))\geq g(n)+\Phi_{\activeg'}(\vect{h}(n))=F^u(n)$. 

[[AF: say a few words about eager?]]

[[AF: also I am wondering whether there are better functions than min?]]
