\label{sec:aggregating}

%We denote by \kastarphi the instantiation of \kastar that uses a given aggregation function $\Phi$. 
In the rest of this section, we examine under which assumptions on $\Phi$ and on the heuristics $\vect{h}$ we can obtain optimality guarantees on \kastarphi. As we shall see, there is a tradeoff between the two types of assumptions: making stronger assumptions on the heuristics leads to a
larger class of aggregation functions that guarnatee optimality. %optimality for a larger range of aggregation functions. [[AF: not sure what the sentence means: it means optimallity]][[AS: Better now?]

As a preliminary, we provide the following definitions and proof several invariant that hold in \kastar regardless of how the heuristics and $\Phi$ are computed.

\begin{definition}
  An algorithm for the \kgs is \emph{admissible} if in any instance, for all reachable goals $t_i$, the first path found to $t_i$ is optimal.
  An algorithm for the \kgs is \emph{$1$-admissible} if in any instance, there exists a goal $t_i$ such that the first path found to $t_i$ is optimal.
  [[Roni: do we need this 1-admissible? I rather we called this admissible]]
\end{definition}
\abda{check 1-admissible definition in case some goals are not reachable}

%Since we defined \kgs for graphs with non-negative edge costs, we limit the discussion to non-negative heuristic functions and correspondingly heuristic aggregation functions that accept non-negative values. Formally,   

\begin{lemma}
  \label{lem:simple}
  In every iteration of \kastarphi, for every active goal $t_i$, there exists a state $n$ in \open on an optimal path to $t_i$, i.e., $g(n) + d(n, t_i) = d(s, t_i)$.
\end{lemma}

Lemma~\ref{lem:simple} can be proven by induction over the iterations of \kastarphi. 
Namely, it trivially holds in the first iteration and continues to hold in subsequent iterations because when a node with $g(\cdot) + d(s,\cdot) = d(s, t_i)$ is expanded then one of its children must also have $g(\cdot) + d(s,\cdot) = d(s, t_i)$.
An equivalent to Lemma~\ref{lem:simple} has been proven for many other best-first search algorithms.

%\begin{lemma}[Completeness]
%  \label{lem:completeness}
%  If a goal is reachable, then it is eventually expanded by \kastar. 
  %[[AF: but not necessarily optimal]
%\end{lemma}
%Note that Lemma~\ref{lem:completeness} does not say that the optimal path to each goal will be found. This requires some restrictions over the heuristics and aggregation function used, as we discuss next. [[Roni: repetition]]

\subsection{Consistent Heuristics}

We begin our exploration by considering the case where all the $k$ heuristic functions are \emph{consistent} (Definition~\ref{def:consistent}). 


\begin{definition}
A heuristic aggregation function $\Phi$ is \emph{\axiomcons} iff for every pair of vectors $\vect{v}$ and $\vect{u}$, we have that if there exists $i$ such that  $u_i = 0$ then $\Phi(\vect{v}) - \Phi(\vect{u}) \leq \max (\vect{v}-\vect{u})$, where $u_i$ we refer the $i^{th}$ element in $\vect{u}$.
\end{definition} 
%By $u_i$ we refer the $i^{th}$ element in $\vect{u}$. 

%\abda{Alternative names: \emph{metric}, \emph{weak contraction}. See \url{https://en.wikipedia.org/wiki/Metric_map} for inspiration.}
% Similar to \cite{Denardo1967}
\begin{theorem}
  \label{thm:consistent}
  Let $\Phi$ be a \axiomcons heuristic aggregation function.
  For any \kgs instance and for any tuple of consistent heuristics $\vect{h}$, \kastarphi is $k$-admissible.
\end{theorem}
\begin{proof}
  Assume that \kastar chooses to expand a goal $t_i \in \{t_1, \ldots t_k\}$) via a path $p$. 
  Applying Lemma~\ref{lem:simple} to $t_i$, we obtain that there exists $n\in \open$ such that $g(n) + d(n, t_i) = d(s, t_i)$.
  Since $t_i$ is expanded before $n$, we have 
  \begin{equation}
      g(t_i) + \Phi(\vect{h}(t_i)) = F_\Phi(t_i) \leq F_\Phi(n) = g(n) + \Phi(\vect{h}(n))
  \end{equation}
    Since all heuristics are consistent, we have that for every $j$
  \begin{align}
    h_j(n)                  & \leq d(n, t_i) + h_j(t_i)   \\
    h_j(n) - h_j(t_i)       & \leq d(n, t_i)               
%    \Phi(\vect{h}(n)) - \Phi(\vect{h}(t_i)) & \leq d(n, t_i) & \text{($\Phi$ is \axiomcons and $h_i(t_i) = 0$)}\\
\end{align}
Since this holds for every $j$, we have: 
\begin{align}
    \max(\vect{h}(n) - \vect{h}(t_i)) & \leq d(n, t_i)      & \\
    \Phi(\vect{h}(n))-\Phi(\vect{h}(t_i))  & \leq d(n, t_i)  & \text{($\Phi$ is \axiomcons and $h_i(t_i) = 0$)}\\
        \Phi(\vect{h}(n))  & \leq d(n, t_i) + \Phi(\vect{h}(t_i)) & \\
    g(n) + \Phi(\vect{h}(n))           & \leq g(n)+ d(n, t_i) + \Phi(\vect{h}(t_i)) & \\
    F_\Phi(n)           & \leq d(s, t_i) + \Phi(\vect{h}(t_i)) &
    \text{(by definition of $n$)}
  \end{align}  
    Now, since $t_i$ is expanded before $n$, we have that $F(t_i) \leq F(n)$. 
  \begin{align}  
    F_\Phi(t_i) \leq F_\Phi(n)                     & \leq d(s, t_i) + \Phi(\vect{h}(t_i)) & \text{($t_i$ is expanded before $n$)}\\
    g(t_i) + \Phi(\vect{h}(t_i))           & \leq d(s, t_i) + \Phi(\vect{h}(t_i)) & \text{(by definition of $F_\Phi$)}\\
    g(t_i)                & \leq d(s, t_i) & 
    \label{eq:consistent}
  \end{align}
  By definition of $d$, we know that $d(s,t_i)\leq g(t_i)$. Therefore, when we expand a goal its $g$ value is equal to $d(s, t_i)$ as required.
\end{proof}

Theorem~\ref{thm:consistent} shows that being \axiomcons is a sufficient condition for optimality of \kastarphi with consistent heuristics.
Our next result shows that it is a necessary condition. 
\begin{figure}
  \centering
  \subfloat{
  \begin{tikzpicture}
    \node[source] (s) at (0, 2) {$s$};
    \node[other]  (a) at (4, 2) {$n$};
    \node[target1] (b) at (4, 0) {$t_i$};
    \node[target2] (c) at (0, 0) {$t_j$};

    \draw[->] (s) -- (a) node[midway] (sa) {\phantom{0}};
    \draw[->] (s) -- (b) node[midway] (sb) {\phantom{0}};
    \draw[->] (s) -- (c) node[midway] (sc) {\phantom{0}};
    \draw[->] (a) -- (b) node[midway] (ab) {\phantom{0}};

    \node[above = -2mm of sa,edgeweight] {$\epsilon$};
    \node[below left = -5mm of sb,edgeweight] {$\delta + 2\epsilon$};
    \node[left  = -2mm of sc,edgeweight] {$\Phi(\vect{v})$};
    \node[right = -2mm of ab,edgeweight] {$\delta$};

    \node[left  = 2mm of  s,infobox] {$\vect{h}=\vect{0}$};
    \node[right = 2mm of  a,infobox] {$\vect{h}=\vect{v}$};
    \node[right = 2mm of  b,infobox] {$\vect{h}=\vect{u}$};
    \node[left  = 2mm of  c,infobox] {$\vect{h}=\vect{0}$};
  \end{tikzpicture}
  } \hfill
  \subfloat{
    \begin{tabular}[b]{lccl}
      \toprule
      Node & $g$ & $\Phi(\vect{h})$ & $F$\\
      \midrule
      $t_i$ & $\delta + 2\epsilon$ & $\Phi(\vect{u})$ & $\Phi(\vect{v}) - \epsilon$ \\
      $t_j$ & $\Phi(\vect{v})$ & $0$ & $\Phi(\vect{v})$\\
      $n$ & $\epsilon$ & $\Phi(\vect{v})$ & $\Phi(\vect{v}) + \epsilon$ \\
      \bottomrule
    \end{tabular}
  }
  \caption[Generic counter-example for \kastarphi]{A generic counter-example for \kastarphi. 
  The start is $s$, nodes $t_i$ and $t_j$ are goals where $i\neq j$. For any, $\vect{v}$, $\vect{u}$, $i$, $\delta$, and $\epsilon$ such that (1) $u_i = 0$, (2) $\delta > 0$, (3) $\Phi(\vect{v}) > \Phi(\vect{u}) + \delta$, and (4) $\epsilon = \frac{\Phi(v) - \Phi(\vect{u}) - \delta}{3} > 0$, running \kastarphi will return a suboptimal path to $t_i$.   To see this, the table on the right shows the $g$, $\Phi(\vect{h})$, and $F_\Phi$ values for $n$, $t_i$, and $t_j$ after expanding $s$. The optimal path to $t_i$ is through $n$ and costs $\delta+\epsilon$, but \kastarphi will expand $t_i$ before $n$, returning a path of length $\delta+2\epsilon$ to $t_i$, which is suboptimal.}
  \label{fig:kstarphi-bad}
\end{figure}
\begin{theorem}
  \label{thm:consistent-dual}
  If a heuristic aggregation function $\Phi$ is not \axiomcons, then there exists a \kgs instance and a tuple of consistent heuristics such that \kastarphi is not $k$-admissible.
\end{theorem}
\begin{proof}
Since $\Phi$ is not \axiomcons, there exists vectors $\vect{v}$, $\vect{u}$, and an index $i$ such that $u_i = 0$ and $\Phi(\vect{v}) - \Phi(\vect{u}) > \max (\vect{v} - \vect{u})$. 
  Set $\delta$ to be a value such that $\Phi(\vect{v}) - \Phi(\vect{u}) > \delta > \max (\vect{v} - \vect{u})$. 
  Set $\epsilon = \frac{\Phi(\vect{v}) - \Phi(\vect{u}) - \delta}{3}$. Observe that $\delta>0$ since the domain of $\Phi$ is vectors of non-negative values (Definition~\ref{def:aggregation}), $v_i \geq 0$, and hence $\max (\vect{v} - \vect{u}) \geq v_i - u_i \geq 0$.
  Also, $\epsilon>0$, by definition of $\delta$. 
  	
  %Define $\epsilon = \frac{\Phi(\vect{v}) - \Phi(\vect{u}) - \delta}{3} > 0$.
  Using $\vect{v}$, $\vect{u}$, $i$, $\delta$, and $\epsilon$, we can construct a counter-example showing that \kastarphi is not $k$-admissible. 
  This counter example is given in Figure~\ref{fig:kstarphi-bad}. 
$s$ is the start, $t_i$ and $t_j$ are the $i$\textsuperscript{th} and $j$\textsuperscript{th} goals, respectively, where $i\neq j$.  
The table in the righthand side of the figure shows the $g$, $\Phi(\vect{h})$, and $F_\Phi$ of the nodes $t_i$, $t_j$, and $n$ after $s$ is expanded. As can be seen, the optimal path to $t_i$ is through $n$, but $t_i$ will be expanded before $n$. Thus, a suboptimal path to $t_i$ will be returned.  
\roni{Why do we need $t_j$?}

  To conclude the proof, it remains to observe that the heuristics involved are consistent.
  This is indeed the case because for all $i'$, we have $h_{i'}(n) - h_{i'}(t_i) = v_{i'} - u_{i'} \leq \max (\vect{v} - \vect{u}) < \delta = d(n, t_i)$ and therefore $h_{i'}(n) \leq d(n, t_i) + h_{i'}(t_i)$.
\end{proof}
Next, we define a broad class of heuristic aggregation function that are \axiomcons. A \emph{subconvex combination} of a vector $\vect{v}$ is a weighted sum $\sum_i \alpha_i\cdot v_i$
such that every coefficients $\alpha_i$ is non-negative
and their sum is at most one, i.e., $\sum_i \alpha_i=1$. 


\begin{lemma}
	\label{thm:subconvex}
	If a heuristic aggregation function $\Phi$ that is a subconvex combination of its input vector
	then $\Phi$ is \axiomcons. 
\end{lemma}
\begin{proof}
	For any two vectors $\vect{v}$ and $\vect{u}$, we have $\Phi(\vect{v}) - \Phi(\vect{u}) = \sum_{1 \leq i \leq k} \alpha_i (v_i - u_i)$. Observe that for every $i$ it holds that $v_i - u_i \leq \max (\vect{v} - \vect{u})$. Thus, 
	$\Phi(\vect{v})-\Phi(\vect{u})\leq \sum_{1\leq i\leq k} \alpha_i \max (\vect{v} - \vect{u}) = \max (\vect{v} - \vect{u})$, as required. 
\end{proof}

\roni{Why I re-worded the above and changed it to a Lemma: The above provides sufficient conditions for a heuristic aggregation function to be cons-compatible. However, we already have a mathematical definition of cons-compatible, so why are we providing a second one. So, I re-worded it to make it clear that this is just a helper lemma, to help prove that many other functions are indeed cons compatible}

\begin{corollary}
	%Using any of the \emph{mean}, the \emph{maximum}, the \emph{minimum}, the \emph{$i$th projection}, for $1 \leq i \leq k$, or the \emph{median} of the values in $\vect{v}$ gives a \axiomcons heuristic aggregation function.
Projection of a single element in a vector, 
\emph{Mean}, \emph{maximum}, and \emph{minimum}, and are all \axiomcons heuristic aggregation function.
\label{cor:mean-etc-cons}
\end{corollary}
\begin{proof}
	Each of these functions can be expressed as a subconvex combinations of the input vector: 
	projecting the $i^{th}$ element is $\Phi(\vect{v})=v_i$; the \emph{mean} is $\Phi(\vect{v}) = \frac{v_1 + \dots + v_k}{k}$; the \emph{maximum} is $\Phi(\vect{v})= v_{(k)}$; and the \emph{minimum} is $\Phi(\vect{v}) = v_{(1)}$. 
	Thus, according to Lemma~\ref{thm:subconvex} they are all \axiomcons. 
\end{proof}
It also possible to extend Corollary~\ref{cor:mean-etc-cons} to show that the median can also be a heuristic aggregation function. 
% We note that ; and the \emph{median} is $\Phi(\vect{v}) = \frac{1}{2}(v_{(\frac{k}{2})} + v_{(\frac{k+1}{2})})$

Some heuristic aggergation function, however, are not \axiomcons, and thus cannot be used in \kastar safely without any additional restrictions. 

\begin{observation}
	If a heuristic aggregation function $\Phi(\vect{v})=\sum_i \alpha_i v_i$ is a linear combination such that all coefficients are non-negative but their sum is larger than one, then $\Phi$ is not \axiomcons
	\label{obs:subconvex-needed}
\end{observation}
\begin{proof}
	Assume that indeed the sum of the coefficients $\alpha_i$ is indeed larger than one, i.e., $\sum_i \alpha_i > 1$.
	Now, consider the vectors $\vect{v} = \vect{1}$ and $\vect{u} = \vect{0}$. Their difference  	
	$\vect{v} - \vect{u} = \sum_i \alpha_i (1 - 0) = \sum_i \alpha_i > 1 = \max (\vect{v} - \vect{w})$. Thus, $\Phi$ is not \axiomcons
\end{proof}
A prime example of an aggregation function that is captured by Observation~\ref{obs:subconvex-needed} is the \emph{sum} function: $\Phi(\vect{v})=\sum_i v_i$, i.e., all the coefficients are one. Therefore, their sum is over one and the resulting function is not a subconvex combination. Due to Observation~\ref{obs:subconvex-needed}, this means sum is not \axiomcons and cannot be safely used in as a heuristic aggregation function for \kastar. 





\subsection{Admissible but Inconsistent Heuristics}
So far we assumed that the $k$ heuristics are consistent. This is a strong requirement in some cases, and there highly effective heurisitcs that are admissible but not consistent~\cite{felner}. Relaxing the requirement for consistency requires stricter restrictions on the heuristic aggregation function, as described below. 



\begin{definition}
	A heuristic aggregation function $\Phi$ is \emph{\axiomadm} iff for every vector $\vect{v}$, we have $\Phi(\vect{v}) \leq \min \vect{v}$.
\end{definition}


%[[AF: adm-compat???]]
%\begin{observation}
%	$\Phi = 0$ and $\Phi=\min$ are \axiomadm.
%\end{observation}

\begin{theorem}
	\label{thm:admissible}
	Let $\Phi$ be a \axiomadm heuristic aggregation function.
	For any \kgs instance and for any tuple of admissible heuristics $\vect{h}$, \kastarphi is $k$-admissible.
\end{theorem}
\begin{proof}
	Assume that \kastarphi chooses to expand a goal $t_i\in\{t_1,\ldots t_k\}$.
	Due to Lemma~\ref{lem:simple}, there exists $n\in \open$ such that $g(n) + d(n, t_i) = d(s, t_i)$.
	
	\begin{align}
	h_{t_i}(n) & \leq d(n,t_i) & \text{($h_{t_i}$ is admissible)}\\
	\Phi(\vect{h}(n)) \leq \min_{t_j} h_{t_j}(n) & \leq d(n, t_i) & \text{($\Phi$ is \axiomadm)}\\
	g(n) + \Phi(\vect{h}(n)) & \leq g(n) + d(n, t_i) = d(s, t_i) & \text{(by definition of $n$)}\\
	F(t_i) \leq F(n) & \leq d(s, t_i) & \text{($t_i$ is expanded before $n$)}\\
	g(t_i) + \Phi(\vect{h}(t_i)) & \leq d(s, t_i) & \text{(by definition of $F$)}\\
	g(t_i) & \leq d(s, t_i) & \text{($\Phi$ is non-negative)}
	%    \label{eq:admissible}
	\end{align}
By definition of $d$, we know that $d(s,t_i)\leq g(t_i)$. Therefore, when we expand a goal its $g$ value is equal to $d(s, t_i)$ as required
\end{proof}

Theorem~\ref{thm:admissible} shows that being \axiomadm is a sufficient condition for optimality of \kastarphi with admissible heuristics.
Our next result shows that it is also a necessary condition.

\begin{theorem}
	\label{thm:admissible-dual}
	If a heuristic aggregation function $\Phi$ is not \axiomadm, then there exists a \kgs instance and a tuple of admissible heuristics such that \kastarphi is not $k$-admissible.
\end{theorem}
\begin{proof}
	This proof relies again on the \kgs instance depicted in Figure~\ref{fig:kstarphi-bad}. Since $\Phi$ is not \axiomadm, there exists a vector $\vect{v}$ such that $\Phi(\vect{v}) > \min \vect{v}$. 
	Let $\vect{u} = \vect{0}$, $i = \argmin \vect{v}$, and $\delta$ such that $\Phi(\vect{v}) > \delta > \min \vect{v} = v_i$.
	Note that $\Phi(\vect{v}) > \Phi(\vect{u}) + \delta$ and consider the counter-example in Figure~\ref{fig:kstarphi-bad}.
	\roni{Abdallah, I don't follow the last line: why is this inequality true? what do we assume about $\Phi(\vect{0})$?}
	
	To conclude the proof, it remains to observe that the heuristics involved are admissible.
	This is indeed the case because we have $h_{t_i}(A) \leq \min \vect{v} \leq \delta = d(A, B)$ and therefore $h_{t_i}(A) \leq d(A, B)$.
\end{proof}

[[AF: doesn't this mean that MIN is the only reasonable function that is k-admissible? Anything smaller than min is not reasonable in my option. It is like cutting a heuristic down]]
\roni{Hmm. Need to think this one this one}


\subsection{Arbitrary Heuristics}
%\abda{$\Phi$ needs to be constant $0$.}

\begin{definition}
  \label{def:ucs}
  \ac{kUCS} is the \kastarvar{0} algorithm where the heuristic aggregation function is the constant 0 function which maps any vector to value 0.
\end{definition}
\ac{kUCS} runs a best-first search without a heuristic, prioritizing nodes based on their $g$ values alone, basically running Dijkstra's algorithm~\cite{DIJ59,Felner2011}, until all goals are expanded.  %As a result, UCS finds the optimal path to every reachable state, including every reachable goal---it is $k$-admissible.

\begin{theorem}
  \label{thm:arbitrary}
  For any \kgs instance and for any tuple of heuristics, \ac{kUCS} is $k$-admissible.
\end{theorem}

\ac{kUCS} does not use any heuristic information and is guaranteed to find the optimal path for each goal. However, without imposing any restriction on the heuristic functions, \ac{kUCS} is the only \kastar instantiation that is $k$-admissible with arbitrary heuristics. 
For example, let $k=1$ and assume that the heuristic functions is inadmissible. Clearly, \kastar will return a suboptimal solution. We 

\begin{theorem}
  \label{thm:arbitrary-dual}
  If a heuristic aggregation function $\Phi$ is not the constant 0, then there exists a \kgs instance and a tuple of arbitrary heuristics such that \kastarphi is not $k$-admissible.
\end{theorem}
\begin{proof}
  $\Phi$ is not the constant function 0, so there exists a vector $\vect{v}$, such that $\Phi(\vect{v}) > 0$,
  Let $\vect{w} = \vect{0}$, $i = 1$, and $\delta$ such that $\Phi(\vect{v}) > \delta > 0$.
  Note that $w_i = 0$ and that $\Phi(\vect{v}) > \Phi(\vect{w}) + \delta = \delta$ and consider the counter-example in Figure~\ref{fig:kstarphi-bad}. [[AF: not clear what are the different variables? nodes? just variables? Also, I think the text in the caption should be placed in the main text]][[AF: I suggest to move all the vector from bold to a letter with a bar above or bellow]]
\end{proof}




